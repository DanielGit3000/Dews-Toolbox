### INSTALL AI SYSTEM:
curl -fsSL https://ollama.com/install.sh | sh
ollama serve
ollama pull llama3.2:latest
ollama list
ollama run llama3.2:latest

### INSTALL WEB UI:
sudo apt-get install docker.io
sudo docker pull ghcr.io/open-webui/open-webui:main
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main

### CHANGE OLLAMA SERVICE
daniel@debian:~$ sudo nano /etc/systemd/system/ollama.service
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
#Environment="PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
Environment="OLLAMA_HOST=0.0.0.0"

[Install]
WantedBy=default.target

### RESTART OLLAMA SERVICE
systemctl daemon-reload
systemctl restart ollama

### CREATE USER, CHANGE OLLAMA HOST, ADD MODEL
connect to http://172.27.173.76:3000
create user via sign in button
go to admin pannel/setting
go to connection and disable openapi connection 
go to connection and change server ip of ollama server and verify connection
go to dashboard and select you model

### DONE 
test you AI 


###################### SOME NOTES, DO NOT USE IT...

WEB UI (without gpu support):
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama

WEB UI (with gpu support):
docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama

WEB UI (without gpu support and ollama on different server):
docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://172.27.170.240:11434 -v open-webui-remote:/app/backend/data --name open-webui-remote --restart always ghcr.io/open-webui/open-webui:main
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main

